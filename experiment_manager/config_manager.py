"""
Licensed Materials - Property of IBM
Restricted Materials of IBM
20221069
Â© Copyright IBM Corp. 2022 All Rights Reserved.
"""
import sys
sys.path.append('../')
import experiment_manager.ibmfl_cli_automator.run as ibmfl_runner
import json
import os
import subprocess
import yaml
import pandas as pd


class ConfigManager:
    """
    The ConfigManager class contains all non-UI logic from the dashboard, necessary to populate objects necessary
    for invoking the runner module.
    """

    def __init__(self):
        self.file_for_supported_combinations = 'supported_models.csv'
        self.file_for_hyperparams = 'hyperparams_to_models_map.json'

        # dict to store choices made via Notebook UI
        self.nb_config = {'split': {}}
        # set defaults
        self.nb_config['split']['ppp'] = 100
        self.nb_config['split']['method'] = 'Uniform Random Sampling'
        self.nb_config['parties'] = 5
        self.nb_config['quorum'] = 1
        self.nb_config['record_metrics'] = False

        # Store all supported datasets, models and algorithms in a pandas dataframe
        self.df = pd.read_csv(filepath_or_buffer=self.file_for_supported_combinations, header=0,
                         names=['fusion_identifier', 'fusion_algo', 'dataset', 'model_spec_name', 'fl_model',
                                'model_ui'], skipinitialspace=True)
        self.df_hyperparams = pd.read_json(path_or_buf=self.file_for_hyperparams)

        self.uimodel_modelid_dict = {
            'Keras': 'keras',
            'PyTorch': 'pytorch',
            'TensorFlow': 'tf',
            'Scikit-learn': 'sklearn',
            'None': 'None'
        }

        # dict to store details such as machines to run on, paths etc
        self.run_details = {}

        self.exp_runner = ibmfl_runner.Runner()

    def generate_update_configs(self):
        # Get timestamp and add it to the given local staging directory:
        self.nb_config['timestamp_str'] = self.exp_runner.generate_timestamp()
        trial_dir = self.run_details['experiments'][0]['local_staging_dir'] + '/' + self.nb_config['timestamp_str']

        # Create the staging_directory:
        mkdir_cmd = 'mkdir -p ' + trial_dir
        process = subprocess.run(mkdir_cmd, shell=True,
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE)
        if process.returncode != 0:
            print('Erred: ', process.stderr)
            return None, None

        if 'custom_data' in self.nb_config:
            self.move_uploaded_files_to_trial_dir(trial_dir)

        if 'custom_data' not in self.nb_config:
            # Generate Data
            print('Generating Data...')

            cmd_to_run = 'cd ../; python3 examples/generate_data.py --num_parties ' + str(
                self.nb_config['parties']) + ' -d ' + self.nb_config['dataset'] + ' -pp ' + str(
                self.nb_config['split']['ppp']) + ' -p ' + trial_dir  # there's only one trial for now
            if 'Stratified' in self.nb_config['split']['method']:
                cmd_to_run = cmd_to_run + ' --stratify'

            # print('Executing {}'.format(cmd_to_run))
            process = subprocess.run(cmd_to_run, shell=True,
                                     stdout=subprocess.PIPE,
                                     stderr=subprocess.PIPE)
            if process.returncode != 0:
                print('Erred: ', process.stderr)
                return None, None

            # path to get datasets from
            data_path = str(process.stdout).split('Data saved in')[-1].strip().replace('\\n\'', '')
            print('Data files saved to: {}'.format(data_path))

        # Generate Configs:
        print('Generating Configs...')
        if 'crypto' in self.nb_config['fusion_identifier']:
            # if it has either of crypto keras or crypto_multiclass_keras, we need -crypto flags:
            # Todo: Need to let user pick one of {Paillier, ThresholdPaillier}
            cmd_to_run = 'cd ../; python3 examples/generate_configs.py' + \
                         ' --num_parties ' + str(self.nb_config['parties']) + \
                         ' -f ' + self.nb_config['fusion_identifier'] + \
                         ' -m ' + self.uimodel_modelid_dict[self.nb_config['model']] + \
                         ' -crypto Paillier' + \
                         ' --config_path ' + trial_dir  # there's only one trial for now
        else:
            cmd_to_run = 'cd ../; python3 examples/generate_configs.py' + \
                         ' --num_parties ' + str(self.nb_config['parties']) + \
                         ' -f ' + self.nb_config['fusion_identifier'] + \
                         ' -m ' + self.uimodel_modelid_dict[self.nb_config['model']] + \
                         ' --config_path ' + trial_dir  # there's only one trial for now

        # add -d and -p flags accordingly
        if 'custom_data' in self.nb_config:
            cmd_to_run = cmd_to_run + ' -d custom_dataset -p "" '# we replace the path down below anyway
        else:
            cmd_to_run = cmd_to_run + ' -d ' + self.nb_config['dataset'] + ' -p ' + data_path

        # print('Executing {}'.format(cmd_to_run))
        process = subprocess.run(cmd_to_run, shell=True,
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE,
                                 universal_newlines=True)
        if process.returncode == 0:
            # save agg and party configs path
            configs_path = os.path.dirname(process.stdout.split('\n')[0].split(':')[1].strip())
            path_to_save_agg_configs = configs_path + '/config_agg.yml'
            print('Aggregator configs saved to: {}'.format(path_to_save_agg_configs))
            path_to_save_party_configs = configs_path + '/config_party*.yml'
            print('Party configs saved to: {}'.format(path_to_save_party_configs))
        else:
            print('Erred: ', process.stderr)
            return None, None

        # modify hyperparameter text to fix quotes
        hyp_text = self.nb_config['global']
        # Python uses True/False, while JSON does true/false
        if 'True' in hyp_text:
            hyp_text = json.loads(hyp_text.replace('\'', '"').replace('True', 'true'))
        elif 'False' in hyp_text:
            hyp_text = json.loads(hyp_text.replace('\'', '"').replace('False', 'false'))
        else:
            hyp_text = json.loads(hyp_text.replace('\'', '"'))
        if 'plus' in self.nb_config['fusion_identifier']:
            rho = hyp_text['rho']
        self.nb_config['global'] = hyp_text

        if 'local' in self.nb_config.keys():
            hyp_text = self.nb_config['local']
            # Python uses True/False, while JSON does true/false
            if 'True' in hyp_text:
                hyp_text = json.loads(hyp_text.replace('\'', '"').replace('True', 'true'))
            elif 'False' in hyp_text:
                hyp_text = json.loads(hyp_text.replace('\'', '"').replace('False', 'false'))
            else:
                hyp_text = json.loads(hyp_text.replace('\'', '"'))

            if 'plus' in self.nb_config['fusion_identifier']:
                alpha = hyp_text['training'].pop('alpha')
            self.nb_config['local'] = hyp_text

        # add num_parties as a key under global, to match the structure in the agg yaml configs
        val = self.nb_config.pop('parties')
        self.nb_config['global']['num_parties'] = val
        val = self.nb_config.pop('quorum')
        self.nb_config['global']['perc_quorum'] = val

        # Load Aggregator Config
        with open(path_to_save_agg_configs, 'r') as stream:
            try:
                agg_config = yaml.safe_load(stream)
            except yaml.YAMLError as e:
                print(e)
                return None, None

        # for local runs, update the dirs to all the "machines" (they're all local)
        if self.run_details['isLocalRun']:
            self.run_details['machines']['ibmfl_dir'] = self.run_details['experiments'][0]['local_ibmfl_dir']
            self.run_details['machines']['staging_dir'] = self.run_details['experiments'][0]['local_staging_dir']

        # Modify aggregator config with values captured from the UI:
        # - update the hyperparameters object with newer global and local objects as updated above
        # - update ip and port from the run_details object
        # - update data handler path to reflect custom datahandler code, if chosen/provided
        agg_config['hyperparams']['global'] = self.nb_config['global']
        if 'local' in self.nb_config.keys():
            agg_config['hyperparams']['local'] = self.nb_config['local']
        agg_machine = self.run_details['experiments'][0]['agg_machine']  # there's only one trial for now

        if not self.run_details['isLocalRun']:
            agg_config['connection']['info']['ip'] = self.run_details['machines'][agg_machine]['ip_address']
            agg_config['connection']['info']['port'] = int(self.run_details['machines'][agg_machine]['port_number'])
            # Todo: support custom dataset for remote runs
        else:
            self.run_details['machines'][agg_machine]['ip_address'] = agg_config['connection']['info']['ip']
            self.run_details['machines'][agg_machine]['port_number'] = agg_config['connection']['info']['port']
            self.run_details['machines'][agg_machine]['ssh_username'] = os.getenv('USER')

        if 'custom_model' in self.nb_config and 'model' in agg_config:
            dst = self.move_model_file_to_trial_dir(agg_config)
            agg_config['model']['spec']['model_definition'] = dst

        # Write this updated yaml to file
        with open(path_to_save_agg_configs, 'w') as out:
            yaml.safe_dump(agg_config, out, default_flow_style=False)
        print('Updated Aggregator config at {}'.format(path_to_save_agg_configs))

        # Modify party config with values accepted from the UI
        # - update IP address, port for agg and party as received from the UI (only remote runs)
        # - add metrics section (both remote and local run) -- if needed
        # - add alpha, if model chosen is Fed+
        # - update data handler path to reflect custom datahandler code, if chosen/provided
        if not self.run_details['isLocalRun']:
            currParty = 0
            for eachMachine in self.run_details['experiments'][0]['party_machines']:  # there's only one trial for now
                # Load
                with open(path_to_save_party_configs.replace('*', str(currParty))) as stream:
                    try:
                        party_config = yaml.safe_load(stream)
                    except yaml.YAMLError as e:
                        print(e)
                        return None, None

                agg_machine = self.run_details['experiments'][0]['agg_machine']  # there's only one trial for now
                # Modify
                party_config['aggregator']['ip'] = self.run_details['machines'][agg_machine]['ip_address']
                party_config['aggregator']['port'] = self.run_details['machines'][agg_machine]['port_number']

                party_config['connection']['info']['ip'] = self.run_details['machines'][eachMachine]['ip_address']
                party_config['connection']['info']['port'] = int(
                    self.run_details['machines'][eachMachine]['port_number'])
                party_config['connection']['info']['port'] = int(
                    self.run_details['machines'][eachMachine]['port_number'])
                # Todo: DRY!
                if self.nb_config['record_metrics']:
                    # Metrics section to add to each party config
                    party_config['metrics_recorder'] = {}
                    party_config['metrics_recorder']['name'] = 'MetricsRecorder'
                    party_config['metrics_recorder']['path'] = 'ibmfl.party.metrics.metrics_recorder'
                    party_config['metrics_recorder']['output_file'] = '${config_dir}/metrics_party${id}'.replace(
                        '${config_dir}', self.run_details['machines'][eachMachine]['staging_dir']).replace('${id}',
                                                                                                          str(currParty))
                    party_config['metrics_recorder']['output_type'] = 'json'
                    party_config['metrics_recorder']['compute_pre_train_eval'] = False
                    party_config['metrics_recorder']['compute_post_train_eval'] = True

                if self.nb_config['fusion_identifier'] == 'fedavgplus': # Todo: CoMed+ and GeoMed+?
                    party_config['local_training']['info']['alpha'] = alpha
                    party_config['local_training']['info']['rho'] = rho

                if 'custom_data' in self.nb_config.keys():
                    party_config['data']['name'] = self.nb_config['custom_data']['name']
                    party_config['data']['path'] = self.nb_config['custom_data']['dh_path']
                    file_ext = self.nb_config['custom_data']['data_path']['party' + str(currParty)].split('.')[-1]
                    if file_ext == 'npz':
                        party_config['data']['info']['npz_file'] = \
                            self.nb_config['custom_data']['data_path']['party' + str(currParty)]
                    else:
                        party_config['data']['info']['txt_file'] = \
                            self.nb_config['custom_data']['data_path']['party' + str(currParty)]
                if 'custom_model' in self.nb_config:
                    # assuming all generated party configs have a model section
                    dst = self.move_model_file_to_trial_dir(party_config)
                    party_config['model']['spec']['model_definition'] = dst

                # Finally, write updated party config to file
                with open(path_to_save_party_configs.replace('*', str(currParty)), 'w') as out:
                    yaml.safe_dump(party_config, out, default_flow_style=False)
                currParty += 1
                # Todo: support custom dataset for remote runs
        else:
            currParty = 0
            for eachMachine in self.run_details['experiments'][0]['party_machines']:  # there's only one trial for now
                # Load
                with open(path_to_save_party_configs.replace('*', str(currParty))) as stream:
                    try:
                        party_config = yaml.safe_load(stream)
                    except yaml.YAMLError as e:
                        print(e)
                        return None, None

                # save IP addr and port number from the party config, into `run_details` dict, for runner's use
                self.run_details['machines'][eachMachine]['ip_address'] = party_config['connection']['info']['ip']
                self.run_details['machines'][eachMachine]['port_number'] = party_config['connection']['info']['port']
                self.run_details['machines'][eachMachine]['ssh_username'] = os.getenv('USER')

                if self.nb_config['record_metrics']:
                    # Metrics section to add to each party config
                    party_config['metrics_recorder'] = {}
                    party_config['metrics_recorder']['name'] = 'MetricsRecorder'
                    party_config['metrics_recorder']['path'] = 'ibmfl.party.metrics.metrics_recorder'
                    party_config['metrics_recorder']['output_file'] = '${config_dir}/metrics_party${id}'.replace(
                        '${config_dir}', trial_dir).replace('${id}', str(currParty))
                    party_config['metrics_recorder']['output_type'] = 'json'
                    party_config['metrics_recorder']['compute_pre_train_eval'] = False
                    party_config['metrics_recorder']['compute_post_train_eval'] = True

                if self.nb_config['fusion_identifier'] == 'fedplus':
                    party_config['local_training']['info']['alpha'] = alpha

                if 'custom_data' in self.nb_config.keys():
                    party_config['data']['name'] = self.nb_config['custom_data']['name']
                    party_config['data']['path'] = self.nb_config['custom_data']['dh_path']
                    file_ext = self.nb_config['custom_data']['data_path']['party' + str(currParty)].split('.')[-1]
                    if file_ext == 'npz':
                        party_config['data']['info']['npz_file'] = \
                            self.nb_config['custom_data']['data_path']['party' + str(currParty)]
                    else:
                        party_config['data']['info']['txt_file'] = \
                            self.nb_config['custom_data']['data_path']['party' + str(currParty)]
                if 'custom_model' in self.nb_config:
                    # assuming all generated party configs have a model section
                    dst = self.move_model_file_to_trial_dir(party_config)
                    party_config['model']['spec']['model_definition'] = dst

                # Finally, write updated party config to file
                with open(path_to_save_party_configs.replace('*', str(currParty)), 'w') as out:
                    yaml.safe_dump(party_config, out, default_flow_style=False)

                currParty += 1

        print('Updated Party configs at {}'.format(path_to_save_party_configs))

        self.nb_config['local_conf_dir'] = str(os.path.dirname(path_to_save_agg_configs))

        return path_to_save_agg_configs, path_to_save_party_configs

    def move_uploaded_files_to_trial_dir(self, trial_directory):
        # trial dir was created in caller, so skipping check

        # move provided dataset files:
        dst = os.path.join(trial_directory, 'datasets')
        if not os.path.exists(dst):
            os.makedirs(dst)
        for key in self.nb_config['custom_data']['data_path']:
            src = self.nb_config['custom_data']['data_path'][key]
            os.rename(src, os.path.join(dst, src.split('/')[-1]))
            print('Moved {} to {}'.format(src, dst))

            # update path in nb_config dict:
            self.nb_config['custom_data']['data_path'][key] = os.path.join(dst, src.split('/')[-1])
        # move provided datahandler file, as it doesn't get moved by runner
        src_dh = self.nb_config['custom_data']['dh_path']
        dst_dh = os.path.join(dst, src_dh.split('/')[-1])
        os.rename(src_dh, dst_dh)
        print('Moved {} to {}'.format(src_dh, dst_dh))

        # update path in nb_config dict:
        self.nb_config['custom_data']['dh_path'] = dst_dh

    def move_model_file_to_trial_dir(self, some_config):
        existing_model_def = some_config['model']['spec']['model_definition']
        if os.path.isdir(existing_model_def):
            # as in the case of TF2
            existing_model_file_path = existing_model_def
            # move user provided model file here
            src = self.nb_config['custom_model']
            # print('user provided model_file_path:', src)
            # look for assets/, variables/ and saved_model.pb files in this folder
            assets_dir = os.path.join(src, 'assets')
            variables_dir = os.path.join(src, 'variables')
            model_file_path = os.path.join(src, 'saved_model.pb')
            # copy each of them
            from distutils.dir_util import copy_tree
            copy_tree(assets_dir, os.path.join(existing_model_file_path, 'assets'))
            copy_tree(variables_dir, os.path.join(existing_model_file_path, 'variables'))

            from shutil import copyfile
            copyfile(model_file_path, os.path.join(existing_model_file_path, 'saved_model.pb'))
            print('Contents of {} written to {}'.format(src, existing_model_file_path))

            # return path for the model file to update respective config (same as before in this case)
            return existing_model_file_path

        else:
            # for .h5, .pt, .pickle files
            existing_model_file_path = existing_model_def[:existing_model_def.rfind('/')]
            # remove existing model file
            if os.path.exists(existing_model_def):
                os.remove(existing_model_def)
            # move user provided model file here
            dst = existing_model_file_path
            # print('dst:', dst)
            src = self.nb_config['custom_model']
            # print('src:', src)
            dst = os.path.join(dst, src.split('/')[-1])
            from shutil import copyfile
            copyfile(src, dst)
            print('Moved {} to {}'.format(src, dst))

            # return new path for the model file to update respective config
            return dst
