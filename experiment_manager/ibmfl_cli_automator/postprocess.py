"""
Licensed Materials - Property of IBM
Restricted Materials of IBM
20221069
Â© Copyright IBM Corp. 2022 All Rights Reserved.
"""
#!/usr/bin/env python3

from collections import OrderedDict
import yaml
import dateutil as du
import json
import math
import pprint
pp = pprint.PrettyPrinter(indent=4)
import re
import statistics
from string import Template
import sys

import matplotlib.pyplot as plt
plt.style.use('seaborn')
plt.rcParams.update({'lines.markeredgewidth': 1})
import numpy as np
np.set_printoptions(precision=2, linewidth=100)
import pandas as pd


def json_to_table(json_data):
    """
    Convert metrics json data into a DataFrame for easier processing

    :param json_data: the raw read-in metrics file
    :type l: `dict`
    :return: reorganized subset of the data as DataFrame, based on some conventions
    :rtype: `pandas.DataFrame`
    """
    #TODO: sort the keys in this order
    table_col_order = ['round_no',
                       'pre_train_ts',
                       'pre_train_eval_reward',
                       'post_train_train_reward_mean',
                       'post_train_train_reward_max',
                       'post_train_ts',
                       'post_train_eval_reward',
                       'pre_update_ts',
                       'post_update_ts']
    i = 0
    table_data = []
    for row in json_data:
        flat_dict = {}
        for k1,v1 in row.items():
            if not isinstance(v1, dict):
                flat_dict[k1] = v1
            else:
                for k2,v2 in v1.items():
                    flat_dict['{}:{}'.format(k1,k2)] = v2
        table_data += [flat_dict]
    return pd.DataFrame(table_data)


def parse_party_data(file_path, n_trials, n_parties):
    """
    Read in all data for an experiment into a single dictionary

    :param file_path: path to the metrics file
    :type file_path: `str`
    :param n_trials: the number of trials in the experiment
    :type n_trials: `int`
    :param n_parties: the number of parties for the experiment
    :type n_parties: `int`
    :return: Dictionary with one key per metric, whose values are lists with one element per party, \
    which is a list of trials, each trial a numpy.array with a value for each round of the trial
    :rtype: `dict[str,list[list[numpy.array]]]`
    """
    # each dict key is a list of list of arrays, outer list -> party, inner list -> trial;
    # metadata is just the round number (used to be IPs per party)
    #TODO: un-hard-code this; still treating metadata weirdly
    dat = {}
    dat['metadata'] = []
    metadata_keys = ['round_no']
    for party in range(n_parties):
        for k,v in dat.items():
            if 'metadata' in k:
                dat[k] += [ {} ]
            else:
                dat[k] += [ [] ]
        for trial in range(1, n_trials+1):
            with open(Template(file_path).substitute({
                    'trial': trial,
                    'id': party,
                    'ts': 'latest'
            })) as json_file:
                table = json_to_table(json.load(json_file))
            for k,v in table.items():
                if k in metadata_keys:
                    dat['metadata'][party][k] = table[k].to_numpy()
                elif k not in dat:
                    dat[k] = [ [] ]
                    dat[k][party] += [ table[k].to_numpy() ]
                else:
                    dat[k][party] += [ table[k].to_numpy() ]
    return dat


def offset_method_first(l):
    """
    Example of an offset method that can be passed to the various offset_vals function below; \
    it offsets all values based on the first value in the list

    :param l: the values to offset
    :type l: list
    :return: a new list (NOT a reference to the input list) with the new elements
    :rtype: list
    """
    ret = np.copy(l)
    for i,v in enumerate(l):
        ret[i] -= l[0]
    return ret


def offset_method_delta(l):
    """
    Example of an offset method that can be passed to the various offset_* functions below; \
    it offsets all values based on the previous value in the list

    :param l: the values to offset
    :type l: list
    :return: a new list (NOT a reference to the input list) with the new elements
    :rtype: list
    """
    ret = np.copy(l)
    for i,v in enumerate(l):
        ret[i] -= l[i-1 if i > 0 else 0]
    return ret


def offset_vals(metrics_dict, offset_keys, offset_methods_dict):
    """
    Compute the offset of the values for a set of metrics using the strategies specified

    :param metrics_dict: data for an experiment (output of parse_party_data)
    :type metrics_dict: `dict[list[list[np.array]]]`
    :param offset_keys: the keys in the metrics_dict to apply the offset to
    :type offset_keys: `list[str]`
    :param offset_methods_dict: a label-indexed dictionary of the strategies to use
    :type offset_methods_dict: `dict[str,callable]`
    :return: A reference to the input metrics dictionary
    :rtype: `dict[list[list[np.array]]]`
    """
    p_id = 0
    for offset_method_suffix,offset_method in offset_methods_dict.items():
        for m_key in offset_keys:
            offset_method_key = '{}_{}'.format(m_key, offset_method_suffix)
            m_val = metrics_dict[m_key]
            metrics_dict[offset_method_key] = []
            for p_ind,p_vals in enumerate(m_val):
                metrics_dict[offset_method_key] += [[]]
                for i_ind,i_vals in enumerate(p_vals):
                    metrics_dict[offset_method_key][p_ind] += [[]]
                    metrics_dict[offset_method_key][p_ind][i_ind] = offset_method(i_vals)

    return metrics_dict


def offset_vals_cycle(metrics_dict, offset_keys):
    """
    Compute the offset of the values for a set of metrics using the "delta" strategy; \
    unlike the above function, the keys listed are used in sequence; instead of the offset \
    being computed per-key, the offset is assuming that all values are taken per round \
    in the order they appear in offset_keys

    :param metrics_dict: data for an experiment (output of parse_party_data)
    :type metrics_dict: `dict[list[list[np.array]]]`
    :param offset_keys: the keys in the metrics_dict to apply the offset to in sequence
    :type offset_keys: `list[str]`
    :return: A reference to the input metrics dictionary
    :rtype: `dict[list[list[np.array]]]`
    """
    p_id = 0
    for m_ind,m_key in enumerate(offset_keys):
        m_val1 = metrics_dict[offset_keys[m_ind]]
        m_val2 = metrics_dict[offset_keys[(m_ind+1) % len(offset_keys)]]
        delta_key = '{}_delta'.format(m_key)
        metrics_dict[delta_key] = []
        for p_ind,p_vals1 in enumerate(m_val1):
            metrics_dict[delta_key] += [[]]
            for i_ind,i_vals1 in enumerate(p_vals1):
                metrics_dict[delta_key][p_ind] += [[]]
                if m_ind == len(offset_keys)-1:
                    metrics_dict[delta_key][p_ind][i_ind] = np.roll(m_val2[p_ind][i_ind], -1) - i_vals1
                    metrics_dict[delta_key][p_ind][i_ind][-1] = np.nan
                else:
                    metrics_dict[delta_key][p_ind][i_ind] = m_val2[p_ind][i_ind] - i_vals1

    return metrics_dict


def group_by_iter(metrics_dict):
    """
    Restructure our metrics dictionary to have the last list store all the trials' values \
    for a given iteration, instead of all the iterations' values for a given trial.

    :param metrics_dict: data for an experiment (output of parse_party_data)
    :type metrics_dict: `dict[list[list[np.array]]]`
    :return: A new, reorganized dict
    :rtype: `dict[list[list[np.array]]]`
    """
    # TODO: more pythonic, pandas-thonic, or numpy-thonic way of doing this?
    metrics_gbi = {}
    # look into the metrics...
    for (metric_key, metric_llist) in metrics_dict.items():
        metrics_gbi[metric_key] = []
        # ... for each party...
        for (party_idx, metric_for_party) in enumerate(metric_llist):
            metrics_gbi[metric_key] += [[]]
            # ... for each trial...
            for metric_for_trial in metric_for_party:
                # ... and finally for each iter.
                for (iter_idx, iter_val) in enumerate(metric_for_trial):
                    if len(metrics_gbi[metric_key][party_idx]) <= iter_idx:
                        metrics_gbi[metric_key][party_idx] += [[]]
                    metrics_gbi[metric_key][party_idx][iter_idx] += [iter_val]
    return metrics_gbi


def aggregate_over_trials(metrics_gbi, agg_methods):
    """
    Aggregate the values for all the trials at a given iteration
    Some examples of agg_methods that could be useful: \
      - statistics.mean \
      - statistics.median \
      - max \
      - lambda x: 2.086 * statistics.stdev(x)/math.sqrt(len(x)) if len(x) > 1 else 0 \
      - lambda x: x[0] \
      - random.choice

    :param metrics_gbi: data for an experiment (output of group_by_iter)
    :type metrics_gbi: `dict[list[list[np.array]]]`
    :return: A new, reorganized dict
    :rtype: `dict[str,list[dict[str,list]]]`
    """
    metrics_abt = {}
    for m_key,m_val in metrics_gbi.items():
        metrics_abt[m_key] = []
        for (party_idx, metric_for_party) in enumerate(m_val):
            if m_key == 'metadata':
                metrics_abt[m_key] += [[]]
                continue
            metrics_abt[m_key] += [{
                key: [] for key in agg_methods.keys()
            }]
            for iter_val_list in metric_for_party:
                for key,val in agg_methods.items():
                    if None in iter_val_list:
                        metrics_abt[m_key][-1][key] += [None]
                    else:
                        metrics_abt[m_key][-1][key] += [val(iter_val_list)]
    return metrics_abt


################################################################################################


def plot_metric_vs_x(metrics_dict, x_vals, metric_key, plot_title, output_filepath=None):
    """
    A helper function for plotting metrics vs various x variables

    :param metrics_dict: data for an experiment \
    (output of aggregate_over_trials with 'mean' and 'stderr' functions applied during aggregation)
    :type metrics_dict: `dict[str,list[dict[str,list]]]`
    :param x_vals: one metric's data for use as x variable in plot
    :type x_vals: `list[list[numeric]]`
    :param metric_key: key in metrics_dict use as y variable in plot
    :type metric_key: `str`
    :param plot_title: text to use as title for plot 
    :type plot_title: `str`
    :param output_filepath: path to save plot to, or None (plot will open in a window)
    :type output_filepath: `str`
    :return: None
    """
    for party,metric_dict in enumerate(metrics_dict[metric_key]):
        plt.errorbar(    x = x_vals[party],
                         y = metric_dict['mean'],
                      yerr = metric_dict['stderr'],
                     label = "party {}".format(party), capsize=2)
    plt.title(plot_title)
    plt.xlabel('step')
    plt.ylabel(metric_key)
    plt.legend(loc='lower right')
    if output_filepath:
        plt.savefig(output_filepath)
    else:
        plt.show()
        plt.close()


def plot_metric_vs_time(metrics_dict, metadata_dict, metric_key, plot_title, output_filepath=None):
    """
    Plot the given metric vs time, based on a common timestamp collected as part of metrics

    :param metrics_dict: data for an experiment \
    (output of aggregate_over_trials with 'mean' and 'stderr' functions applied during aggregation)
    :type metrics_dict: `dict[str,list[dict[str,list]]]`
    :param metadata_dict: 'metadata' key from the output of parse_party_data
    :type metadata_dict: `dict`
    :param metric_key: key in metrics_dict use as y variable in plot
    :type metric_key: `str`
    :param plot_title: text to use as title for plot 
    :type plot_title: `str`
    :param output_filepath: path to save plot to, or None (plot will open in a window)
    :type output_filepath: `str`
    :return: None
    """
    party_list = range(len(metrics_dict['post_train:ts_off']))
    x_vals = {party: metrics_dict['post_train:ts_off'][party]['mean'] for party in party_list}
    plot_metric_vs_x(metrics_dict,
                     x_vals,
                     metric_key,
                     plot_title, output_filepath=None)


def plot_metric_vs_round(metrics_dict, metadata_dict, metric_key, plot_title, output_filepath=None):
    """
    Plot the given metric vs round no, which is collected as part of the metrics' metadata

    :param metrics_dict: data for an experiment \
    (output of aggregate_over_trials with 'mean' and 'stderr' functions applied during aggregation)
    :type metrics_dict: `dict[str,list[dict[str,list]]]`
    :param metadata_dict: 'metadata' key from the output of parse_party_data
    :type metadata_dict: `dict`
    :param metric_key: key in metrics_dict use as y variable in plot
    :type metric_key: `str`
    :param plot_title: text to use as title for plot 
    :type plot_title: `str`
    :param output_filepath: path to save plot to, or None (plot will open in a window)
    :type output_filepath: `str`
    :return: None
    """
    party_list = range(len(metadata_dict))
    x_vals = {party: metadata_dict[party]['round_no'] for party in party_list}
    plot_metric_vs_x(metrics_dict,
                     x_vals,
                     metric_key,
                     plot_title, output_filepath=None)

def gen_reward_vs_time_plots2(dat, reward_keys, x_axis_val='round', x_axis_key=None):
    """
    Plot the given metric vs round no, which is collected as part of the metrics' metadata

    :param dat: metrics data as produced by runner.get_experiment_output \
    (the dict has one element, 'metadata', which is a dict as well, contrary to the below type)
    :type dat: `dict[list[list[np.array]]]`
    :param reward_keys: keys to plot on y axis
    :type reward_keys: `list[str]`
    :param x_axis_val: we can specify a label here to select our x-axis value; \
    'round' is treated uniquely to use the round number, any other string requires x_axis_key
    :type x_axis_val: `str`
    :param x_axis_key: the key to use, if we didn't select 'round' for the x_axis_val
    :type x_axis_key: `str`
    :return: None
    """
    # group metrics data by iter, so each inner list contains each trials' value for that iter
    metadata_dict = dat.pop('metadata')
    metrics_dict = group_by_iter(dat)
    trial_agg_methods = {
        'mean': statistics.mean,
        'stderr': lambda x: 2.086 * statistics.stdev(x)/math.sqrt(len(x)) if len(x) > 1 else 0,
        'len': len
    }
    metrics_dict = aggregate_over_trials(metrics_dict, trial_agg_methods)

    # make plots for the desired values
    if x_axis_val == 'round':
        plot_metric = plot_metric_vs_round
    elif x_axis_val == 'time':
        plot_metric = plot_metric_vs_time
    else:
        print('Bad x-axis value.')
        sys.exit(1)

    for reward_key in reward_keys:
        plot_title = '{} vs {}'.format(reward_key, x_axis_val)
        plot_metric(metrics_dict, metadata_dict, reward_key, plot_title)

def gen_reward_vs_time_plots(metrics_file_tmpl, n_trials, n_parties, reward_keys,
                             x_axis_val='round', x_axis_key=None):
    """
    Plot the given metric vs round no, which is collected as part of the metrics' metadata

    :param metrics_file_tmpl: a path to the metrics files for an experiment, \
    containing a ${trial} and ${id} template parameter
    :type metrics_file_tmpl: `str`
    :param n_trials: number of trials in the experiment to use for this plot
    :type n_trials: `int`
    :param n_parties: number of parties in the experiment to use for this plot 
    :type n_parties: `int`
    :param reward_keys: keys in 
    :type reward_keys: `list[str]`
    :param x_axis_val: we can specify a label here to select our x-axis value; \
    'round' is treated uniquely to use the round number, any other string requires x_axis_key
    :type x_axis_val: `str`
    :param x_axis_key: the key to use, if we didn't select 'round' for the x_axis_val
    :type x_axis_key: `str`
    :return: None
    """
    # obtain the party data
    dat = parse_party_data(metrics_file_tmpl, n_trials, n_parties)
    offset_methods = {
            'off': offset_method_first,
            'del': offset_method_delta
        }
    dat = offset_vals(dat, [k for k,v in dat.items() if ':ts' in k], offset_methods)
    # pp.pprint(dat)

    gen_reward_vs_time_plots2(dat, reward_keys, x_axis_val, x_axis_key)


def gen_timing_plots(metrics_file_tmpl, n_trials, n_parties, offset_cycle_keys):
    """
    Plot the timing deltas for the code regions delinteated by a list of keys as a stacked bar

    :param metrics_file_tmpl: a path to the metrics files for an experiment, \
    containing a ${trial} and ${id} template parameter
    :type metrics_file_tmpl: `str`
    :param n_trials: number of trials in the experiment to use for this plot
    :type n_trials: `int`
    :param n_parties: number of parties in the experiment to use for this plot 
    :type n_parties: `int`
    :param offset_cycle_keys: keys of timestamp metrics which delineate regions to time
    :type offset_cycle_keys: `list[str]`
    :return: None
    """
    # obtain the party data
    dat = parse_party_data(metrics_file_tmpl, n_trials, n_parties)
    dat = offset_vals_cycle(dat, offset_cycle_keys)

    # get metadata from dictionary and remove it as it won't be treated the same way as the rest
    metadata_dict = dat.pop('metadata')

    # group metrics data by iter, so each inner list contains each trials' value for that iter
    metrics_dict = group_by_iter(dat)
    trial_agg_methods = {
        'mean': statistics.mean,
        'var': lambda x: statistics.variance(x) if len(x) > 1 else 0,
        'len': len
    }
    metrics_dict = aggregate_over_trials(metrics_dict, trial_agg_methods)

    # filter keys to the only ones we want
    offset_cycle_keys = ['{}_delta'.format(k) for k in offset_cycle_keys]
    metrics_dict = { k: metrics_dict[k] for k in metrics_dict.keys() & offset_cycle_keys }

    # compute mean over rounds, and compute standard error using error propagation
    for k, v in metrics_dict.items():
        for i,p in enumerate(v):
            n_iters = len(p['var'])
            p['mean'] = np.nanmean(p['mean'])
            p['var'] = np.nansum(p['var'])/math.pow(n_iters,2)
            p['sde'] = 2.086 * math.sqrt(p['var'])/math.sqrt(n_iters)
            p['len'] = statistics.mode(p['len'])

    # reorganize values for plotting
    plot_dict = {k: {} for k in offset_cycle_keys}
    for k, v in metrics_dict.items():
        plot_dict[k]['mean'] = []
        plot_dict[k]['sde'] = []
        for party in v:
            print(party)
            plot_dict[k]['mean'] += [party['mean']]
            plot_dict[k]['sde'] += [party['sde']]

    (fig1, axes) = plt.subplots(1, 4, gridspec_kw={'width_ratios': [0.8, 1.9, 3.95, 8]}, sharey=True,
                                constrained_layout=True)
    first = True
    width = 0.85
    kv_prev = [0.0 for x in range(n_parties)]
    print(kv_prev)
    for ki, kv in plot_dict.items():
        axes[1].bar(range(n_parties), kv['mean'], width, yerr=kv['sde'], bottom=kv_prev)
        kv_prev = [kv['mean'][i] + kv_prev[i] for i in range(1)]
    axes[1].set_xlabel('{} parties'.format(n_parties))
    axes[1].set_xticks(range(n_parties))
    axes[1].set_xticklabels(range(1,n_parties+1))
    axes[0].set_ylabel('time (s)')
    fig1.suptitle('Time per Iteration (8 runs, all randomized machines, sorted)')
    plt.show()
    plt.close()


################################################################################################


    """
    This simply shows an example call to each of the two provided plotting functions.

    We can easily perform the following postprocessing tests:
    - varying n_parties / sync_interval / n_workers_per_party:
        - reward vs round 
        - reward vs time
    - variation of timings within party (stacked bar).
    """
if __name__ == '__main__':

    gen_reward_vs_time_plots(
        #'/home/sean/repos/IBMFL_new/IBMFL/examples/data/trial${trial}/party${id}_metrics.json',
        '/tmp/ibmfl_results/trial${trial}/party${id}_metrics.json',
        1, 2,
        ['post_train:eval:episode_reward_mean'])

    #gen_timing_plots(
    #    '/home/sean/repos/IBMFL_new/IBMFL/examples/data/trial{}/party{}_metrics.json',
    #    1, 2,
    #    ['pre_update_ts', 'post_update_ts', 'pre_train_ts', 'post_train_ts'])
