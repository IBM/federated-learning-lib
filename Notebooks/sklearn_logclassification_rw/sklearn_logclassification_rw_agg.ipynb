{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness in IBM FL: Scikitlearn Logistic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Paper: Mitigating Bias in Federated Learning \n",
    "\n",
    "Check it out [here](https://arxiv.org/abs/2012.02447)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline:\n",
    "- [Add conda environment to Jupyter Notebook](#setup)\n",
    "- [FL and Fairness](#intro)\n",
    "- [Aggregator](#Aggregator)\n",
    "    - [Aggregator Configuration](#Aggregator-Configuration)\n",
    "    - [Running the Aggregator](#Running-the-Aggregator)\n",
    "- [Starting Parties](#Starting-Parties)\n",
    "- [Training and Evaluation](#Training-and-Evaluation)\n",
    "- [Visualize Results](#Visualize-Results)\n",
    "- [Shut Down](#Shut-Down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add conda environment to Jupyter Notebook <a name=\"setup\"></a>\n",
    "\n",
    "Please ensure that you have activated the `conda` environment following the instructions in the project README.\n",
    "\n",
    "Once done, run the following commands in your terminal to install your conda environment into the Jupyter Notebook:\n",
    "\n",
    "1. Once you have activated the conda environment, install the `ipykernel` package: `conda install -c anaconda ipykernel`\n",
    "\n",
    "2. Next, install the `ipykernel` module within Jupyter Notebook: `python -m ipykernel install --user --name=<conda_env>`\n",
    "\n",
    "3. Please install the `matplotlib` package for your conda environment.\n",
    "\n",
    "4. Finally, restart the jupyter notebook once done. Ensure that you are running this Notebook from `<project_path>/Notebooks`, where project_path is the directory where the IBMFL repository was cloned.\n",
    "\n",
    "When the Notebook is up and running it may prompt you to choose the kernel. Use the drop down to choose the kernel name same as that chosen when running `conda activate <conda_env>`. If no prompt shows up, you can change the kernel by clicking _Kernel_ > _Change kernel_ > _`<conda_env>`_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning (FL) and Fairness <a name=\"intro\"></a>\n",
    "\n",
    "**Federated Learning (FL)** is a distributed machine learning process in which each participant node (or party) retains their data locally and interacts with  other participants via a learning protocol. In this notebook, we demonstrate the adaption and usage of popular **bias mitigation techniques** for FL. We examine bias from the perspective of social fairness, as opposed to contribution fairness.\n",
    "\n",
    "Bias mitigation approaches in machine learning mainly measure and reduce undesired bias with respect to a *sensitive attribute*, such as *age* or *race*, in the training dataset. \n",
    "\n",
    "We utilize [IBM FL](https://github.com/IBM/federated-learning-lib) to have multiple parties train a classifier to predict whether a person in the [Adult dataset](http://archive.ics.uci.edu/ml/datasets/Adult) makes over $50,000 a year. We have adapted 2 centralized fairness methods, Reweighing and Prejudice Remover, into 3 federated learning bias mitigation methods: Local Reweighing, Global Reweighing with Differential Privacy, and Federated Prejudice Removal. With these methods, we can run a variety of fairness experiments in IBM FL.\n",
    "\n",
    "For a more technical dive into IBM FL, refer the whitepaper [here](https://arxiv.org/pdf/2007.10987.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Techniques <a name=\"fairness\"></a>\n",
    "\n",
    "[Reweighing](https://link.springer.com/article/10.1007/s10115-011-0463-8) is a centralized pre-processing bias mitigation method, which works primarily by attaching weights to samples in the training dataset. This method accesses the entire training dataset and computes weights as the ratio of the expected probability to the observed probability of the sample, calculated based on the sensitive attribute/label pairing in question. We adapt this centralized method into two federated learning techniques, Local Reweighing and Global Reweighing with Differential Privacy.\n",
    "\n",
    "**Local reweighing**: To fully protect parties' data privacy, each party computes reweighing weights locally based on its own training dataset during pre-processing and then uses the reweighing dataset for its local training. Therefore, parties do not need to communicate with the aggregator or reveal their sensitive attributes and data sample information.\n",
    "\n",
    "**Global Reweighing with Differential Privacy**: If parties agree to share sensitive attributes and noisy data statistics, parties can employ this fairness method. During the pre-processing phase, the aggregator will collect statistics such as the noisy number of samples with privileged attribute values, compute global reweighing weights  based on the collected statistics, and share them with parties. By adjusting the amount of noise injected via epsilon, parties can control their data leakage while still mitigating bias. \n",
    "\n",
    "[Prejudice Remover](https://github.com/algofairness/fairness-comparison/tree/master/fairness/algorithms/kamishima) is an in-processing bias mitigation method 440 proposed for centralized ML, which works by adding a fairness-aware regularizer to the regular logistic loss function. We adapt this centralized method into Federated Prejudice Remover.\n",
    "\n",
    "**Federated Prejudice Removal**: Each party applies the Prejudice Remover algorithm to train a less biased local model, and shares only the model parameters with the aggregator. The aggregator can then employ existing FL algorithms, like simple average and FedAvg, etc., to update the global model.\n",
    "\n",
    "Further details about the algorithms and datasets utilized, as well as experimental setup, are included in our [paper](https://arxiv.org/abs/2012.02447)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics <a name=\"mnist\"></a>\n",
    "\n",
    "In fairness evaluation, there is no single, all-inclusive metric. Literature uses multiple metrics to measure several aspects, painting a composition of fairness. We use four highly-utilized fairness metrics: Statistical Parity Difference, Equal Odds Difference, Average Odds Difference, and Disparate Impact.\n",
    "\n",
    "**Statistical Parity Difference**: Calculated as the ratio of the success rate between the unprivileged and privileged groups. The ideal value for this metric is 0, and the fairness range is between -0.1 and 0.1, as defined by [AI Fairness 360](https://aif360.mybluemix.net/).\n",
    "\n",
    "**Equal Odds Difference**: Calculated as the true positive rate difference between the unprivileged and privileged groups. The ideal value for this metric is 0, and the fairness range is between -0.1 and 0.1, similarly defined by AI Fairness 360.\n",
    "\n",
    "**Average Odds Difference**: Calculated as the mean of the false positive rate difference and the true positive rate difference, both between the unprivileged and privileged groups. The ideal value for this metric is 0, and the fairness range is between -0.1 and 0.1, similarly defined by AI Fairness 360.\n",
    "\n",
    "**Disparate Impact**: Calculated as the difference of the success rate between the unprivileged and privileged groups. The ideal value for this metric is 1, and the fairness range is between 0.8 and 1.2, similarly defined by AI Fairness 360."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting things ready\n",
    "We begin by setting the number of parties that will participate in the federated learning run and splitting up the data among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "os.chdir(\"../..\")\n",
    "\n",
    "num_parties = 2  ## number of participating parties\n",
    "dataset = 'adult'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `examples/generate_data.py` to split the dataset into files for each party. \n",
    "\n",
    "The script allows specifying the number of parties as well as the dataset to use (from several supported datasets: _mnist_, _femnist_, _cifar10_ and many others). \n",
    "\n",
    "The `-pp` argument states how many data points to choose per party. If the option `--stratify` is given, the library stratifies the data proportionally according to the source distribution. If you want to run this notebook in different machines, you can assign samples for each party locally. Then, we define the neural network definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run examples/generate_data.py -n $num_parties -d $dataset -pp 200 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and generate the sklearn model file and save it locally. Please note that parties data and the model file needs to be copied to the parties if you launch parties on different nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "folder_configs = 'examples/configs/sklearn_logclassification_rw'\n",
    "\n",
    "model = SGDClassifier(loss='log', penalty='l2')\n",
    "\n",
    "if not os.path.exists(folder_configs):\n",
    "    os.makedirs(folder_configs)\n",
    "\n",
    "fname = os.path.join(folder_configs, 'model_architecture.pickle')\n",
    "\n",
    "with open(fname, 'wb') as f:\n",
    "    joblib.dump(model, f)\n",
    "\n",
    "print(\"Model file is generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregator coordinates the overall process, communicates with the parties and integrates the results of the training process. This integration of results is done using the _Fusion Algorithm_.\n",
    "\n",
    "A fusion algorithm queries the registered parties to carry out the federated learning process. The queries sent vary according to the model/algorithm type.  In return, parties send their reply as a model update object, and these model updates are then aggregated according to the specified Fusion Algorithm, specified via a `Fusion Handler` class. \n",
    "\n",
    "To take a look at the supported fusion algorithms, refer the IBM FL tutorial page [here](https://github.com/IBM/federated-learning-lib/blob/main/README.md#supported-functionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregator Configuration\n",
    "\n",
    "We discuss the various configuration parameters for the Aggregator [here.](https://github.com/IBM/federated-learning-lib/blob/main/docs/tutorials/configure_fl.md#the-aggregators-configuration-file) Given below is an example of the aggregator's configuration file for the **Local Reweighing** method, which uses the **AdultSklearnDataHandler**, the **ReweighLocalTrainingHandler**, and the **IterAvgFusionHandler**.\n",
    "\n",
    "To run the Global Reweighing with Differential Privacy method, we would use the **AdultSklearnDataHandler**, the **ReweighLocalTrainingHandler**, and the **ReweighFusionHandler**.\n",
    "\n",
    "To run the Federated Prejudice Removal method, we would use the **AdultPRDataHandler**, the **PRLocalTrainingHandler**, the **SklearnPRFLModel**, and the **PrejRemoverFusionHandler**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/arch_aggregator.png\" width=\"680\"/>\n",
    "<figcaption><center>Image Source: <a href=\"https://arxiv.org/pdf/2007.10987.pdf\">IBM Federated Learning: An Enterprise FrameworkWhite Paper V0.1</a></center></figcaption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agg_config = {\n",
    "    'connection': {\n",
    "        'info': {\n",
    "            'ip': '127.0.0.1',\n",
    "            'port': 5000,\n",
    "            'tls_config': {\n",
    "                'enable': 'false'\n",
    "            }\n",
    "        },\n",
    "        'name': 'FlaskConnection',\n",
    "        'path': 'ibmfl.connection.flask_connection',\n",
    "        'sync': 'False'\n",
    "    },\n",
    "    'data': {\n",
    "        'info': {\n",
    "            'txt_file': 'examples/datasets/adult.data'\n",
    "        },\n",
    "        'name': 'AdultSklearnDataHandler',\n",
    "        'path': 'ibmfl.util.data_handlers.adult_sklearn_data_handler'\n",
    "    },\n",
    "    'fusion': {\n",
    "        'name': 'IterAvgFusionHandler',\n",
    "        'path': 'ibmfl.aggregator.fusion.iter_avg_fusion_handler'\n",
    "    },\n",
    "    'hyperparams': {\n",
    "        'global': {\n",
    "            'rounds': 10,\n",
    "            'termination_accuracy': 0.9\n",
    "        },\n",
    "        'local': {\n",
    "            'training': {\n",
    "                'max_iter': 2\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'protocol_handler': {\n",
    "        'name': 'ProtoHandler',\n",
    "        'path': 'ibmfl.aggregator.protohandler.proto_handler'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Aggregator\n",
    "Next we pass the configuration parameters set in the previous cell to instantiate the `Aggregator` object. Finally, we `start()` the Aggregator process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from ibmfl.aggregator.aggregator import Aggregator\n",
    "aggregator = Aggregator(config_dict=agg_config)\n",
    "\n",
    "aggregator.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/arch_party.png\" width=\"680\"/>\n",
    "<figcaption><center>Image Source: <a href=\"https://arxiv.org/pdf/2007.10987.pdf\">IBM Federated Learning: An Enterprise FrameworkWhite Paper V0.1</a></center></figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Parties\n",
    "\n",
    "Now that we have Aggregator running, next we go to Parties' notebooks (`sklearn_logclassification_rw_p0.ipynb` and `sklearn_logclassification_rw_p1.ipynb`) to start and register them with the Aggregator. Once all the parties are done with registration, we will move to next step to start training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Now that our network has been set up, we begin training the model by invoking the Aggregator's `start_training()` method. \n",
    "\n",
    "This could take some time, depending on your system specifications. Feel free to get your doze of coffee meanwhile â˜•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#1 Initialize the metrics collector variables\n",
    "\"\"\"\n",
    "num_parties = 2\n",
    "eval_party_accuracy = [[] for _ in range(2)]\n",
    "iterations = [[] for _ in range(2)]\n",
    "\n",
    "\"\"\"\n",
    "#2 Register handler for metrics collector\n",
    "\"\"\"\n",
    "def get_metrics(metrics):\n",
    "    keys = list(metrics['party'].keys())\n",
    "    keys.sort()\n",
    "    for i in range(len(keys)):\n",
    "      eval_party_accuracy[i].append(metrics['party'][keys[i]]['acc'])\n",
    "      iterations[i].append(metrics['fusion']['curr_round']*3)\n",
    "      \n",
    "mh = aggregator.fusion.metrics_manager\n",
    "mh.register(get_metrics)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#3 start the training\n",
    "\"\"\"\n",
    "aggregator.start_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut Down\n",
    "\n",
    "Invoke the `stop()` method on each of the network participants to terminate the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Parties' Training\n",
    "Please go to Parties' notebooks to visalize summary of Parties' training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
