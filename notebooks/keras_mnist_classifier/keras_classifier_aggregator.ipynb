{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classifier in IBM FL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline:\n",
    "- [Add conda environment to Jupyter Notebook](#setup)\n",
    "- [Federated Learning(FL)](#intro)\n",
    "- [Digit Recognition](#mnist)\n",
    "- [Aggregator](#Aggregator)\n",
    "    - [Aggregator Configuration](#Aggregator-Configuration)\n",
    "    - [Running the Aggregator](#Running-the-Aggregator)\n",
    "- [Starting Parties](#Starting-Parties)\n",
    "- [Training and Evaluation](#Training-and-Evaluation)\n",
    "- [Visualize Results](#Visualize-Results)\n",
    "- [Shut Down](#Shut-Down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add conda environment to Jupyter Notebook <a name=\"setup\"></a>\n",
    "\n",
    "Please ensure that you have activated the `conda` environment following the instructions in the project README.\n",
    "\n",
    "Once done, run the following commands in your terminal to install your conda environment into the Jupyter Notebook:\n",
    "\n",
    "1. Once you have activated the conda environment, install the `ipykernel` package: `conda install -c anaconda ipykernel`\n",
    "\n",
    "2. Next, install the `ipykernel` module within Jupyter Notebook: `python -m ipykernel install --user --name=<conda_env>`\n",
    "\n",
    "3. Please install the `matplotlib` package for your conda environment.\n",
    "\n",
    "4. Finally, restart the jupyter notebook once done. Ensure that you are running this Notebook from `<project_path>/Notebooks`, where project_path is the directory where the IBMFL repository was cloned.\n",
    "\n",
    "When the Notebook is up and running it may prompt you to choose the kernel. Use the drop down to choose the kernel name same as that chosen when running `conda activate <conda_env>`. If no prompt shows up, you can change the kernel by clicking _Kernel_ > _Change kernel_ > _`<conda_env>`_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning (FL) <a name=\"intro\"></a>\n",
    "\n",
    "**Federated Learning (FL)** is a distributed machine learning process in which each participant node (or party) retains their data locally and interacts with  other participants via a learning protocol. \n",
    "One main driver behind FL is the need to not share data with others  due to privacy and confidentially concerns.\n",
    "Another driver is to improve the speed of training a machine learning model by leveraging other participants' training processes.\n",
    "\n",
    "Setting up such a federated learning system requires setting up a communication infrastructure, converting machine learning algorithms to federated settings and in some cases knowing about the intricacies of security and privacy enabling techniques such as differential privacy and multi-party computation. \n",
    "\n",
    "In this Notebook we use [IBM FL](https://github.com/IBM/federated-learning-lib) to have multiple parties train a classifier to recognise handwritten digits in the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). \n",
    "\n",
    "For a more technical dive into IBM FL, refer the whitepaper [here](https://arxiv.org/pdf/2007.10987.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we set up each of the components of a Federated Learning network (See Figure below) wherein all involved parties aid in training their respective local cartpoles to arrive at the upright pendulum state. In this notebook we default to 2 parties, but depending on your resources you may use more parties.\n",
    "\n",
    "<img style=\"display=block; margin:auto\" src=\"../images/FL_Network.png\" width=\"720\"/>\n",
    "<p style=\"text-align: center\">Modified from Image Source: <a href=\"https://arxiv.org/pdf/2007.10987.pdf\">IBM Federated Learning: An Enterprise FrameworkWhite Paper V0.1</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognition <a name=\"mnist\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"display=block; margin:auto\" src=\"../images/MnistExamples.png\" width=\"512\"/>\n",
    "<p style=\"text-align: center\">Image Source: Josef Steppan / CC BY-SA <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">Wikimedia Commons</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem at hand is to recognize digits from these tens of thousands of handwritten images. In this notebook, we are going to assume that the parties and the aggregator are run in the same machine. For that purpose, we first randomly split the training data to each party. Then, we define the neural network definition. After that we start the aggregator and we need to register and run all parties by running two other notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting things ready\n",
    "We begin by setting the number of parties that will participate in the federated learning run and splitting up the data among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "os.chdir(\"../..\")\n",
    "\n",
    "num_parties = 2  ## number of participating parties\n",
    "dataset = 'mnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `examples/generate_data.py` to split the dataset into files for each party. \n",
    "\n",
    "The script allows specifying the number of parties as well as the dataset to use (from several supported datasets: _mnist_, _femnist_, _cifar10_ and many others). \n",
    "\n",
    "The `-pp` argument states how many data points to choose per party. If the option `--stratify` is given, the library stratifies the data proportionally according to the source distribution. If you want to run this notebook in different machines, you can assign samples for each party locally. Then, we define the neural network definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run examples/generate_data.py -n $num_parties -d $dataset -pp 200 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the keras model definition file using the below function save_model_config. Please note that parties data and the model file needs to be copied to the parties if you launch parties on different nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "def save_model_config(folder_configs):\n",
    "    num_classes = 10\n",
    "    img_rows, img_cols = 28, 28\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    if not os.path.exists(folder_configs):\n",
    "        os.makedirs(folder_configs)\n",
    "\n",
    "    # Save model\n",
    "    fname = os.path.join(folder_configs, 'compiled_keras.h5')\n",
    "    model.save(fname)\n",
    "    print(fname)\n",
    "\n",
    "    K.clear_session()\n",
    "    # Generate model spec:\n",
    "    spec = {\n",
    "        'model_name': 'keras-cnn',\n",
    "        'model_definition': fname\n",
    "    }\n",
    "\n",
    "    model = {\n",
    "        'name': 'KerasFLModel',\n",
    "        'path': 'ibmfl.model.keras_fl_model',\n",
    "        'spec': spec\n",
    "    }\n",
    "\n",
    "    return model\n",
    "save_model_config('examples/configs/keras_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coordinates the overall process, communicates with the parties and integrates the results of the training process. This integration of results is done using the _Fusion Algorithm_.\n",
    "\n",
    "A fusion algorithm queries the registered parties to carry out the federated learning process. The queries sent vary according to the model/algorithm type.  In return, parties send their reply as a model update object, and these model updates are then aggregated according to the specified Fusion Algorithm, specified via a `Fusion Handler` class. \n",
    "\n",
    "To take a look at the supported fusion algorithms, refer the IBM FL tutorial page [here](https://github.com/IBM/federated-learning-lib/blob/main/README.md#supported-functionality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregator Configuration\n",
    "\n",
    "We discuss the various configuration parameters for the Aggregator [here.](https://github.com/IBM/federated-learning-lib/blob/main/docs/tutorials/configure_fl.md#the-aggregators-configuration-file)\n",
    "\n",
    "Given below is an example of the aggregator's configuration file. In this example, the aggregator does not specify a data file or maintain a global model. Hence, during the federated learning process, it only keeps track of the current model parameters. \n",
    "\n",
    "However, it is possible that the aggregator also has data for testing purposes and maintains a global model. When this is the case, one needs to add `data` and `model` sections in the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"display=block; margin:auto\" src=\"../images/arch_aggregator.png\" width=\"680\"/>\n",
    "<p style=\"text-align: center\">Image Source: <a href=\"https://arxiv.org/pdf/2007.10987.pdf\">IBM Federated Learning: An Enterprise FrameworkWhite Paper V0.1</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building blocks in the configuration file:\n",
    "\n",
    "- `connection` (also used in Party config): needed to initiate the connection; includes server information (`info`, optional, depends on the connection type), a connection class name, a connection file location, and a synchronization mode flag (`sync`) for training phase\n",
    "\n",
    "- `fusion`: needed to initiate a fusion algorithm at the aggregator side; includes a fusion handler class name and a fusion handler file location\n",
    "\n",
    "- `protocol_handler`: needed to initiate a protocol; includes a protocol handler class name and a protocol handler file location\n",
    "\n",
    "- `hyperparams`: includes global training and local training hyperparameters, including `termination_accuracy`, learning rate (`lr`), `optimizer`, etc., depending on the model being used.\n",
    "\n",
    "For detailed documentation of the objects described below, refer the API docs [here](https://ibmfl.mybluemix.net/api-documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agg_config = {\n",
    "    'connection': {\n",
    "        'info': {\n",
    "            'ip': '127.0.0.1',\n",
    "            'port': 5000,\n",
    "            'tls_config': {\n",
    "                'enable': 'false'\n",
    "            }\n",
    "        },\n",
    "        'name': 'FlaskConnection',\n",
    "        'path': 'ibmfl.connection.flask_connection',\n",
    "        'sync': 'False'\n",
    "    },\n",
    "    'data': {\n",
    "        'info': {\n",
    "            'npz_file': 'examples/datasets/mnist.npz'\n",
    "        },\n",
    "        'name': 'MnistKerasDataHandler',\n",
    "        'path': 'ibmfl.util.data_handlers.mnist_keras_data_handler'\n",
    "    },\n",
    "    'fusion': {\n",
    "        'name': 'IterAvgFusionHandler',\n",
    "        'path': 'ibmfl.aggregator.fusion.iter_avg_fusion_handler'\n",
    "    },\n",
    "    'hyperparams': {\n",
    "        'global': {\n",
    "            'max_timeout': 60,\n",
    "            'num_parties': num_parties,\n",
    "            'perc_quorum': 1,\n",
    "            'rounds': 30,\n",
    "            'termination_accuracy': 0.9\n",
    "        },\n",
    "        'local': {\n",
    "            'optimizer': {\n",
    "                'lr': 0.01\n",
    "            },\n",
    "            'training': {\n",
    "                'epochs': 3\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'protocol_handler': {\n",
    "        'name': 'ProtoHandler',\n",
    "        'path': 'ibmfl.aggregator.protohandler.proto_handler'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Aggregator\n",
    "Next we pass the configuration parameters set in the previous cell to instantiate the `Aggregator` object. Finally, we `start()` the Aggregator process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from ibmfl.aggregator.aggregator import Aggregator\n",
    "aggregator = Aggregator(config_dict=agg_config)\n",
    "\n",
    "aggregator.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"display=block; margin:auto\" src=\"../images/arch_party.png\" width=\"680\"/>\n",
    "<p style=\"text-align: center\">Image Source: <a href=\"https://arxiv.org/pdf/2007.10987.pdf\">IBM Federated Learning: An Enterprise FrameworkWhite Paper V0.1</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Parties\n",
    "\n",
    "Now that we have Aggregator running, next we go to Parties' notebooks (`keras_classifier_p0.ipynb` and `keras_classifier_p1.ipynb`) to start and register them with the Aggregator. Once all the parties are done with registration, we will move to next step to start training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Now that our network has been set up, we begin training the model by invoking the Aggregator's `start_training()` method. \n",
    "\n",
    "This could take some time, depending on your system specifications. Feel free to get your dose of coffee meanwhile ☕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#1 Initialize the metrics collector variables\n",
    "\"\"\"\n",
    "num_parties = agg_config['hyperparams']['global']['num_parties']\n",
    "eval_party_accuracy = [[] for _ in range(num_parties)]\n",
    "iterations = [[] for _ in range(num_parties)]\n",
    "\n",
    "\"\"\"\n",
    "#2 Register handler for metrics collector\n",
    "\"\"\"\n",
    "def get_metrics(metrics):\n",
    "    keys = list(metrics['party'].keys())\n",
    "    keys.sort()\n",
    "    for i in range(len(keys)):\n",
    "      eval_party_accuracy[i].append(metrics['party'][keys[i]]['acc'])\n",
    "      iterations[i].append(metrics['fusion']['curr_round']*agg_config['hyperparams']['local']['training']['epochs'])\n",
    "      \n",
    "mh = aggregator.fusion.metrics_manager\n",
    "mh.register(get_metrics)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#3 start the training\n",
    "\"\"\"\n",
    "aggregator.start_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#1 define a method to plot a smooth cure\n",
    "\"\"\"\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "import numpy as np\n",
    "\n",
    "def smooth_curve(xaxis, yaxis):\n",
    "   xnew = np.linspace(min(xaxis), max(xaxis), len(xaxis)*50) \n",
    "\n",
    "   spl = make_interp_spline(xaxis, yaxis, k=1)  # type: BSpline\n",
    "   power_smooth = spl(xnew)\n",
    "   return xnew, power_smooth \n",
    "\n",
    "f = plt.figure(figsize=(20,10))\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "num_parties = agg_config['hyperparams']['global']['num_parties']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#2 plot the evaluation curve\n",
    "\"\"\"\n",
    "for i in range(num_parties):\n",
    "    eval_party_xaxis, eval_party_yaxis = smooth_curve(iterations[i], eval_party_accuracy[i])\n",
    "    line1, = plt.plot(eval_party_xaxis, eval_party_yaxis)\n",
    "    line1.set_label('party'+str(i+1))\n",
    "plt.title('Evaluation Plot')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut Down\n",
    "\n",
    "Invoke the `stop()` method on each of the network participants to terminate the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Parties' Training\n",
    "Please go to Parties' notebooks to visalize summary of Parties' training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
