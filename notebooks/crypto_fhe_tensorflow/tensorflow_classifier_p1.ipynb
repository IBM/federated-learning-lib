{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classifier with HELayers (HE) - Secure Aggregation in IBM FL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline:\n",
    "- [Add conda environment to Jupyter Notebook](#setup)\n",
    "- [Federated Learning(FL)](#intro)\n",
    "- [How does IBM FL work with HE?](#HE-FL)\n",
    "- [Digit Recognition](#mnist)\n",
    "- [Parties](#Parties)\n",
    "    - [Party Configuration](#Party-Configuration)\n",
    "    - [Party Setup](#Party-Setup)\n",
    "- [Register All Parties Before Starting Training](#Register-All-Parties-Before-Starting-Training)\n",
    "- [Visualize Results](#Visualize-Results)\n",
    "- [Shut Down](#Shut-Down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add conda environment to Jupyter Notebook <a name=\"setup\"></a>\n",
    "\n",
    "Please ensure that you have activated the `conda` environment following the instructions in the project README.\n",
    "\n",
    "Once done, run the following commands in your terminal to install your conda environment into the Jupyter Notebook:\n",
    "\n",
    "1. Once you have activated the conda environment, install the `ipykernel` package: `conda install -c anaconda ipykernel`\n",
    "\n",
    "2. Next, install the `ipykernel` module within Jupyter Notebook: `python -m ipykernel install --user --name=<conda_env>`\n",
    "\n",
    "3. Please install the `matplotlib` package for your conda environment. \n",
    "\n",
    "4. Finally, restart the jupyter notebook once done. Ensure that you are running this Notebook from `<project_path>/notebooks/`, where project_path is the directory where the IBMFL repository was cloned.\n",
    "\n",
    "When the Notebook is up and running it may prompt you to choose the kernel. Use the drop down to choose the kernel name same as that chosen when running `conda activate <conda_env>`. If no prompt shows up, you can change the kernel by clicking _Kernel_ > _Change kernel_ > _`<conda_env>`_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning (FL) <a name=\"intro\"></a>\n",
    "\n",
    "**Federated Learning (FL)** is a distributed machine learning process in which each participant node (or party) retains their data locally and interacts with  other participants via a learning protocol. \n",
    "One main driver behind FL is the need to not share data with others  due to privacy and confidentially concerns.\n",
    "Another driver is to improve the speed of training a machine learning model by leveraging other participants' training processes.\n",
    "\n",
    "Setting up such a federated learning system requires setting up a communication infrastructure, converting machine learning algorithms to federated settings and in some cases knowing about the intricacies of security and privacy enabling techniques such as differential privacy and multi-party computation. \n",
    "\n",
    "In this Notebook we use [IBM FL](https://github.com/IBM/federated-learning-lib) to have multiple parties train a classifier to recognise handwritten digits in the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). \n",
    "\n",
    "For a more technical dive into IBM FL, refer the whitepaper [here](https://arxiv.org/pdf/2007.10987.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we set up each of the components of a Federated Learning network (See Figure below) wherein all involved parties aid in training their respective local cartpoles to arrive at the upright pendulum state. In this notebook we default to 2 parties, but depending on your resources you may use more parties.\n",
    "\n",
    "<img style=\"display=block; margin:auto\" src=\"../images/FL_Network.png\" width=\"720\"/>\n",
    "<p style=\"text-align: center\">Modified from Image Source: <a href=\"https://arxiv.org/pdf/2007.10987.pdf\">IBM Federated Learning: An Enterprise FrameworkWhite Paper V0.1</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does IBM FL work with HE? <a name=\"HE-FL\"></a>\n",
    "\n",
    "IBM FL uses the *[Cheon-Kim-Kim-Song (CKKS) scheme](https://eprint.iacr.org/2016/421.pdf)* for Homomorphic Encryption. HE functionalities are implemented using *[IBM HElayers software development kit (SDK)](https://github.com/IBM/helayers)*, and in particular, its *[PyHElayers](https://github.com/IBM/helayers#pyhelayers-python-package)* Python package. You can install `pyhelayers` in your conda environment by running `pip install pyhelayers`. Note that `pyhelayers` is currently supported only on Linux (x86 and IBM Z).\n",
    "\n",
    "<img style=\"display=block; margin:auto\" src=\"../images/ibmfl_helayer.png\" width=\"512\"/>\n",
    "<p style=\"text-align: center\">IBM FL library integrated with HELayers </p>\n",
    "\n",
    "After enabling IBM FL with HE, parties do not send their model updates in plaintext. Each party sends an encrypted model update and the aggregation is performed under encryption. (See the figure below.) \n",
    "\n",
    "\n",
    "![SegmentLocal](../images/FL_FHE_v3.gif \"segment\")\n",
    "<p style=\"text-align: center\">FL Training with HE</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Recognition <a name=\"mnist\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"display=block; margin:auto\" src=\"../images/MnistExamples.png\" width=\"512\"/>\n",
    "<p style=\"text-align: center\">Image Source: Josef Steppan / CC BY-SA <a href=\"https://creativecommons.org/licenses/by-sa/4.0\">Wikimedia Commons</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem at hand is to recognize digits from these tens of thousands of handwritten images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting things ready\n",
    "We begin by setting the number of parties that will participate in the federated learning run and splitting up the data among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "party_id = 1\n",
    "\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "os.chdir(\"../..\")\n",
    "\n",
    "dataset = 'mnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parties\n",
    "\n",
    "Each party holds its own dataset that is kept to itself and used to answer queries received from the aggregator. Because each party may have stored data in different formats, FL offers an abstraction called Data Handler. This module allows for custom implementations to retrieve the data from each of the participating parties. A local training handler sits at each party to control the local training happening at the party side. \n",
    "\n",
    "<img style=\"display=block; margin:auto\" src=\"../images/FHE_stacks.png\" width=\"680\"/>\n",
    "<p style=\"text-align: center\">Aggregator and Party side configurations</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party Configuration\n",
    "\n",
    "**Note**: in a typical FL setting, the parties may have very different configurations from each other. However, in this simplified example, the config does not differ much across parties. So, we first setup the configuration common to both parties, in the next cell. We discuss the parameters that are specific to each, in subsequent cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Party Setup\n",
    "In the following cell, we setup configurations for parties, including network-level details, hyperparameters as well as the model specifications. Please note that if you are running this notebook in distributed environment on separate nodes then you need to split the data locally and obtain the model h5 generated by the Aggregator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Blocks in Party Configuration:\n",
    "\n",
    "Now we configure party specific configurations in the `get_party_config` method, which specifies model related configurations as well as other parameters necessary for the federated learning setup. The model related configurations are identical to those generated (yaml files) during the terminal run.\n",
    "\n",
    "Once these are done, we invoke them for each party, in the subsequent cell.\n",
    "\n",
    "- `aggregator`: IP and port at which the Aggregator is running, so the party may connect to it\n",
    "\n",
    "- `data`: information needed to initiate a data handler class; includes a given data path, a data handler class name, and a data handler file location\n",
    "\n",
    "- `model`: details about the model, including name, the model class file location, i.e., path, and the given model specification path, i.e., spec. In this example, we use the Tensorflow FL Model class, as indicated under the spec\n",
    "\n",
    "- `local_training`: handles the train and eval commands, also initializes data, environment and models.\n",
    "    + 'name': 'CryptoLocalTrainingHandler', -> This makes sure that the local training handler with HE is invoked (currently, this is the only local training handler with HE that is supported)\n",
    "    + 'path': 'ibmfl.party.training.crypto_fedavg_local_training_handler', -> The path to the previous item\n",
    "    + 'info': inside this field, we specify the crypto options including:\n",
    "        - 'crypto'\n",
    "            + name: specify the crypto system (Use `CryptoFHE` for Homomorphic Encryption. Other crypto systems may be added at a later stage.)\n",
    "            + path: the path to the previous item\n",
    "            + key_manager:\n",
    "                - name: specify the key manager type (This is fixed to `LocalDiskKeyManager` for now. More key management protocols may be added at a later stage.)\n",
    "                - path: the path to the previous item (This is fized to `ibmfl.crypto.keys_mng.crypto_key_mng_dsk` for now.)\n",
    "                - key_mgr_info: specify the paths for the private key file and public key context file\n",
    "\n",
    "\n",
    "- `protocol_handler`: party protocol handler communicates with the Aggregator bridging between Aggregator and local training handler. \n",
    "    - 'name': 'PartyProtocolHandler',\n",
    "    - 'path': 'ibmfl.party.party_protocol_handler', -> The path to the previous item\n",
    "\n",
    "**Note**: in a typical FL setting, the parties may have very different configurations from each other. However, in this simplified example, the config does not differ much across parties. Also, as of this release all parties registered with the aggregator participate in the training. Dynamically letting registered parties to skip training in certain rounds will be supported in subsequent versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ctx_file = os.path.join(os.getcwd(), 'notebooks/fhe.context')\n",
    "key_file = os.path.join(os.getcwd(), 'notebooks/fhe.key')\n",
    "def get_party_config(party_id):\n",
    "    party_config = {\n",
    "        'aggregator':\n",
    "            {\n",
    "                'ip': '127.0.0.1',\n",
    "                'port': 5000\n",
    "            },\n",
    "        'connection': {\n",
    "            'info': {\n",
    "                'ip': '127.0.0.1',\n",
    "                'port': 8085 + party_id,\n",
    "            },\n",
    "            'name': 'FlaskConnection',\n",
    "            'path': 'ibmfl.connection.flask_connection',\n",
    "            'sync': False\n",
    "        },\n",
    "        'data': {\n",
    "            'info': {\n",
    "                'npz_file': 'examples/data/mnist/random/data_party'+ str(party_id) +'.npz'\n",
    "            },\n",
    "            'name': 'MnistTFDataHandler',\n",
    "            'path': 'ibmfl.util.data_handlers.mnist_keras_data_handler'\n",
    "        },\n",
    "        'local_training': {\n",
    "            'name': 'CryptoLocalTrainingHandler',\n",
    "            'path': 'ibmfl.party.training.crypto_local_training_handler',\n",
    "            'info': {\n",
    "                'crypto': {\n",
    "                    'name': 'CryptoFHE',\n",
    "                    'path': 'ibmfl.crypto.helayer.fhe',\n",
    "                    'key_manager': {\n",
    "                        'name': 'LocalDiskKeyManager',\n",
    "                        'path': 'ibmfl.crypto.keys_mng.crypto_key_mng_dsk',\n",
    "                        'key_mgr_info': {\n",
    "                            'files': {\n",
    "                                'context': ctx_file,\n",
    "                                'key': key_file\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'model': {\n",
    "            'name': 'TensorFlowFLModel',\n",
    "            'path': 'ibmfl.model.tensorflow_fl_model',\n",
    "            'spec': {\n",
    "                'model_definition': 'examples/configs/tf_classifier',\n",
    "                'model_name': 'tf-cnn'\n",
    "            }\n",
    "        },\n",
    "        'protocol_handler': {\n",
    "            'name': 'PartyProtocolHandler',\n",
    "            'path': 'ibmfl.party.party_protocol_handler'\n",
    "        }\n",
    "    }\n",
    "    return party_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Party\n",
    "\n",
    "Now, we invoke the `get_party_config` function to setup party and `start()` it.\n",
    "\n",
    "Finally, we register the party with the Aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 20:18:37.937 INFO   numexpr.utils :: Note: NumExpr detected 30 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-11-18 20:18:37.938 INFO   numexpr.utils :: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-18 20:18:38,675 | 1.0.7 | INFO | ibmfl.util.config                             | Getting Aggregator details from arguments.\n",
      "2022-11-18 20:18:38,880 | 1.0.7 | INFO | ibmfl.util.config                             | No metrics recorder config provided for this setup.\n",
      "2022-11-18 20:18:39,051 | 1.0.7 | INFO | ibmfl.util.config                             | No metrics config provided for this setup.\n",
      "2022-11-18 20:18:39,052 | 1.0.7 | INFO | ibmfl.util.config                             | No evidencia recordeer config provided for this setup.\n",
      "2022-11-18 20:18:39,053 | 1.0.7 | INFO | ibmfl.util.data_handlers.mnist_keras_data_handler | Loaded training data from examples/data/mnist/random/data_party1.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 20:18:39.092116: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-18 20:18:39.092364: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 20:18:39.094868: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-18 20:18:39,367 | 1.0.7 | INFO | ibmfl.connection.flask_connection             | RestSender initialized\n",
      "2022-11-18 20:18:39,369 | 1.0.7 | INFO | ibmfl.crypto.crypto_library                   | Initializing a key manager\n",
      "2022-11-18 20:18:39,370 | 1.0.7 | INFO | ibmfl.crypto.helayer.fhe                      | Initializing keys\n",
      "2022-11-18 20:18:39,371 | 1.0.7 | INFO | ibmfl.crypto.helayer.fhe                      | Initializing a FHE Cipher\n",
      "2022-11-18 20:18:39,398 | 1.0.7 | INFO | ibmfl.crypto.helayer.fhe                      | No flag for privacy of fusion weights in config. Setting to default value of False.\n",
      "2022-11-18 20:18:39,401 | 1.0.7 | INFO | ibmfl.connection.flask_connection             | Receiver Initialized\n",
      "2022-11-18 20:18:39,401 | 1.0.7 | INFO | ibmfl.connection.flask_connection             | Initializing Flask application\n",
      "2022-11-18 20:18:39,405 | 1.0.7 | INFO | ibmfl.party.party                             | Party initialization successful\n",
      "2022-11-18 20:18:39,406 | 1.0.7 | INFO | ibmfl.party.party                             | Party start successful\n",
      "2022-11-18 20:18:39,406 | 1.0.7 | INFO | ibmfl.party.party                             | Registering party...\n",
      "2022-11-18 20:18:39,410 | 1.0.7 | INFO | werkzeug                                      |  * Running on http://127.0.0.1:8086/ (Press CTRL+C to quit)\n",
      "2022-11-18 20:18:39,416 | 1.0.7 | INFO | ibmfl.party.party                             | Registration Successful\n",
      "2022-11-18 20:18:49,086 | 1.0.7 | INFO | ibmfl.connection.flask_connection             | Request received for path :7\n",
      "2022-11-18 20:18:49,089 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | received a async request\n",
      "2022-11-18 20:18:49,089 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | finished async request\n",
      "2022-11-18 20:18:49,091 | 1.0.7 | INFO | werkzeug                                      | 127.0.0.1 - - [18/Nov/2022 20:18:49] \"POST /7 HTTP/1.1\" 200 -\n",
      "2022-11-18 20:18:49,094 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Handling async request in a separate thread\n",
      "2022-11-18 20:18:49,094 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Received request from aggregator\n",
      "2022-11-18 20:18:49,095 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Received request in with message_type:  7\n",
      "2022-11-18 20:18:49,096 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Received request in PH 7\n",
      "2022-11-18 20:18:49,096 | 1.0.7 | INFO | ibmfl.party.training.local_training_handler   | No model update was provided.\n",
      "2022-11-18 20:18:49,097 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Local training started...\n",
      "2022-11-18 20:18:49,102 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | Learning rate of optimizer is set as <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "2022-11-18 20:18:49,102 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | Training hps for this round => batch_size: 128, epochs 5, steps_per_epoch None\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 20:18:49.129482: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-11-18 20:18:49.129974: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300080000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 158ms/step - loss: 2.0186 - accuracy: 0.3640\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 1s 150ms/step - loss: 0.8322 - accuracy: 0.8528\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 0.3409 - accuracy: 0.9211\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 1s 129ms/step - loss: 0.1986 - accuracy: 0.9362\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.1263 - accuracy: 0.9672\n",
      "2022-11-18 20:18:52,846 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Local training done, start to encrypt model update...\n",
      "2022-11-18 20:18:52,848 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Encrypting - <class 'ibmfl.crypto.helayer.fhe.CryptoFHE'>\n",
      "2022-11-18 20:18:55,883 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Encryption done.\n",
      "2022-11-18 20:19:00,619 | 1.0.7 | INFO | ibmfl.util.fl_metrics                         | reshaping y_pred\n",
      "2022-11-18 20:19:00,717 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | {'f1 micro': 0.86, 'precision micro': 0.86, 'recall micro': 0.86, 'f1 macro': 0.86, 'precision macro': 0.86, 'recall macro': 0.86, 'f1 weighted': 0.86, 'precision weighted': 0.86, 'recall weighted': 0.86}\n",
      "2022-11-18 20:19:00,719 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | {'loss': 0.4628874659538269, 'acc': 0.86, 'accuracy': 0.8629999756813049, 'f1 micro': 0.86, 'precision micro': 0.86, 'recall micro': 0.86, 'f1 macro': 0.86, 'precision macro': 0.86, 'recall macro': 0.86, 'f1 weighted': 0.86, 'precision weighted': 0.86, 'recall weighted': 0.86}\n",
      "2022-11-18 20:19:00,719 | 1.0.7 | INFO | ibmfl.party.training.local_training_handler   | {'loss': 0.4628874659538269, 'acc': 0.86, 'accuracy': 0.8629999756813049, 'f1 micro': 0.86, 'precision micro': 0.86, 'recall micro': 0.86, 'f1 macro': 0.86, 'precision macro': 0.86, 'recall macro': 0.86, 'f1 weighted': 0.86, 'precision weighted': 0.86, 'recall weighted': 0.86}\n",
      "2022-11-18 20:19:00,720 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | successfully finished async request\n",
      "2022-11-18 20:19:14,457 | 1.0.7 | INFO | ibmfl.connection.flask_connection             | Request received for path :7\n",
      "2022-11-18 20:19:15,634 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | received a async request\n",
      "2022-11-18 20:19:15,635 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | finished async request\n",
      "2022-11-18 20:19:15,636 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Handling async request in a separate thread\n",
      "2022-11-18 20:19:15,643 | 1.0.7 | INFO | werkzeug                                      | 127.0.0.1 - - [18/Nov/2022 20:19:15] \"POST /7 HTTP/1.1\" 200 -\n",
      "2022-11-18 20:19:15,644 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Received request from aggregator\n",
      "2022-11-18 20:19:15,646 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Received request in with message_type:  7\n",
      "2022-11-18 20:19:15,646 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Received request in PH 7\n",
      "2022-11-18 20:19:15,648 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Received encrypted model update.\n",
      "2022-11-18 20:19:15,649 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Decrypting - <class 'ibmfl.crypto.helayer.fhe.CryptoFHE'>\n",
      "2022-11-18 20:19:16,650 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Decryption done.\n",
      "2022-11-18 20:19:16,662 | 1.0.7 | INFO | ibmfl.party.training.local_training_handler   | Local model updated.\n",
      "2022-11-18 20:19:16,666 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Local training started...\n",
      "2022-11-18 20:19:16,705 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | Learning rate of optimizer is set as <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "2022-11-18 20:19:16,738 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | Training hps for this round => batch_size: 128, epochs 5, steps_per_epoch None\n",
      "Epoch 1/5\n",
      "4/4 [==============================] - 1s 152ms/step - loss: 0.1480 - accuracy: 0.9600\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 1s 128ms/step - loss: 0.0943 - accuracy: 0.9740\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.0502 - accuracy: 0.9960\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 98ms/step - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 114ms/step - loss: 0.0189 - accuracy: 0.9980\n",
      "2022-11-18 20:19:19,533 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Local training done, start to encrypt model update...\n",
      "2022-11-18 20:19:19,535 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Encrypting - <class 'ibmfl.crypto.helayer.fhe.CryptoFHE'>\n",
      "2022-11-18 20:19:21,652 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Encryption done.\n",
      "2022-11-18 20:19:25,365 | 1.0.7 | INFO | ibmfl.util.fl_metrics                         | reshaping y_pred\n",
      "2022-11-18 20:19:25,570 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | {'f1 micro': 0.88, 'precision micro': 0.88, 'recall micro': 0.88, 'f1 macro': 0.87, 'precision macro': 0.88, 'recall macro': 0.88, 'f1 weighted': 0.88, 'precision weighted': 0.88, 'recall weighted': 0.88}\n",
      "2022-11-18 20:19:25,574 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | {'loss': 0.4651050865650177, 'acc': 0.88, 'accuracy': 0.8822000026702881, 'f1 micro': 0.88, 'precision micro': 0.88, 'recall micro': 0.88, 'f1 macro': 0.87, 'precision macro': 0.88, 'recall macro': 0.88, 'f1 weighted': 0.88, 'precision weighted': 0.88, 'recall weighted': 0.88}\n",
      "2022-11-18 20:19:25,575 | 1.0.7 | INFO | ibmfl.party.training.local_training_handler   | {'loss': 0.4651050865650177, 'acc': 0.88, 'accuracy': 0.8822000026702881, 'f1 micro': 0.88, 'precision micro': 0.88, 'recall micro': 0.88, 'f1 macro': 0.87, 'precision macro': 0.88, 'recall macro': 0.88, 'f1 weighted': 0.88, 'precision weighted': 0.88, 'recall weighted': 0.88}\n",
      "2022-11-18 20:19:25,576 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | successfully finished async request\n",
      "2022-11-18 20:19:36,715 | 1.0.7 | INFO | ibmfl.connection.flask_connection             | Request received for path :7\n",
      "2022-11-18 20:19:37,897 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | received a async request\n",
      "2022-11-18 20:19:37,898 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | finished async request\n",
      "2022-11-18 20:19:37,898 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Handling async request in a separate thread\n",
      "2022-11-18 20:19:37,905 | 1.0.7 | INFO | werkzeug                                      | 127.0.0.1 - - [18/Nov/2022 20:19:37] \"POST /7 HTTP/1.1\" 200 -\n",
      "2022-11-18 20:19:37,906 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Received request from aggregator\n",
      "2022-11-18 20:19:37,908 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Received request in with message_type:  7\n",
      "2022-11-18 20:19:37,909 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | Received request in PH 7\n",
      "2022-11-18 20:19:37,909 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Received encrypted model update.\n",
      "2022-11-18 20:19:37,910 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Decrypting - <class 'ibmfl.crypto.helayer.fhe.CryptoFHE'>\n",
      "2022-11-18 20:19:38,630 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Decryption done.\n",
      "2022-11-18 20:19:38,670 | 1.0.7 | INFO | ibmfl.party.training.local_training_handler   | Local model updated.\n",
      "2022-11-18 20:19:38,672 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Local training started...\n",
      "2022-11-18 20:19:38,705 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | Learning rate of optimizer is set as <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.001>\n",
      "2022-11-18 20:19:38,736 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | Training hps for this round => batch_size: 128, epochs 5, steps_per_epoch None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 0s 135ms/step - loss: 0.0465 - accuracy: 0.9920\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 121ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 119ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "2022-11-18 20:19:41,411 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Local training done, start to encrypt model update...\n",
      "2022-11-18 20:19:41,434 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Encrypting - <class 'ibmfl.crypto.helayer.fhe.CryptoFHE'>\n",
      "2022-11-18 20:19:43,661 | 1.0.7 | INFO | ibmfl.party.training.crypto_local_training_handler | Encryption done.\n",
      "2022-11-18 20:19:46,707 | 1.0.7 | INFO | ibmfl.util.fl_metrics                         | reshaping y_pred\n",
      "2022-11-18 20:19:46,878 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | {'f1 micro': 0.89, 'precision micro': 0.89, 'recall micro': 0.89, 'f1 macro': 0.88, 'precision macro': 0.89, 'recall macro': 0.89, 'f1 weighted': 0.89, 'precision weighted': 0.89, 'recall weighted': 0.89}\n",
      "2022-11-18 20:19:46,883 | 1.0.7 | INFO | ibmfl.model.tensorflow_fl_model               | {'loss': 0.41456085443496704, 'acc': 0.89, 'accuracy': 0.8917999863624573, 'f1 micro': 0.89, 'precision micro': 0.89, 'recall micro': 0.89, 'f1 macro': 0.88, 'precision macro': 0.89, 'recall macro': 0.89, 'f1 weighted': 0.89, 'precision weighted': 0.89, 'recall weighted': 0.89}\n",
      "2022-11-18 20:19:46,886 | 1.0.7 | INFO | ibmfl.party.training.local_training_handler   | {'loss': 0.41456085443496704, 'acc': 0.89, 'accuracy': 0.8917999863624573, 'f1 micro': 0.89, 'precision micro': 0.89, 'recall micro': 0.89, 'f1 macro': 0.88, 'precision macro': 0.89, 'recall macro': 0.89, 'f1 weighted': 0.89, 'precision weighted': 0.89, 'recall weighted': 0.89}\n",
      "2022-11-18 20:19:46,888 | 1.0.7 | INFO | ibmfl.party.party_protocol_handler            | successfully finished async request\n"
     ]
    }
   ],
   "source": [
    "from ibmfl.party.party import Party\n",
    "import tensorflow as tf\n",
    "\n",
    "party_config = get_party_config(party_id)\n",
    "party = Party(config_dict=party_config)\n",
    "party.start()\n",
    "party.register_party()\n",
    "party.proto_handler.is_private = False  ## allows sharing of metrics with aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register All Parties Before Starting Training\n",
    "\n",
    "Now we have started and registered this Party. Next, we will start and register rest of the parties. Once all the Parties have registered we will go back to the Aggregator's notebook to start training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Here we plot the summary graphs from each party's training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "_, (X_test, Y_test) = mnist.load_data()\n",
    "X_test = X_test[..., tf.newaxis].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score\n",
    "sample_count = 100\n",
    "num_parties = 1\n",
    "for i in range(10):\n",
    "   test_filter = np.where(Y_test == i)\n",
    "   X_test1, Y_test1 = X_test[test_filter], Y_test[test_filter]\n",
    "   #labels, counts = np.unique(Y_test1, return_counts=True)\n",
    "   np.random.seed(123)\n",
    "   rand_v = np.random.randint(0, X_test1.shape[0],sample_count)\n",
    "   test_digits = X_test1[rand_v]\n",
    "   test_labels = Y_test1[rand_v]\n",
    "   labels, counts = np.unique(test_labels, return_counts=True)\n",
    "   #print('Original lable', labels)\n",
    "   acc_list = np.array([])\n",
    "   y_true = np.full(sample_count, i)\n",
    "\n",
    "       \n",
    "   y_pred = np.array([])\n",
    "   for i_samples in range(sample_count):\n",
    "      pred = party.fl_model.predict(test_digits[i_samples].reshape(1, 28, 28, 1))\n",
    "      y_pred = np.append(y_pred, pred.argmax())\n",
    "   acc = accuracy_score(y_true, y_pred) * 100\n",
    "   #print('y prediction',y_pred)\n",
    "   #print('y true',y_true)\n",
    "   #print('accuracy',acc)\n",
    "   acc_list = np.append(acc_list,acc)\n",
    "\n",
    "   ind = np.arange(num_parties) \n",
    "   fig2 = plt.figure(constrained_layout=True,figsize=(10,10))\n",
    "   fig2.tight_layout()\n",
    "   spec2 = gridspec.GridSpec(ncols=6, nrows=4, figure=fig2) \n",
    "   f2_ax1 = fig2.add_subplot(spec2[0:1, 0:2])\n",
    "   f2_ax1.imshow(test_digits[0], cmap='gray')\n",
    "   plt.axis('off')\n",
    "   f2_ax2 = fig2.add_subplot(spec2[0:1, 3:])\n",
    "   labels = [(i+1) for i in range(num_parties)] \n",
    "   \n",
    "   x = np.arange(num_parties)\n",
    "  \n",
    "\n",
    "   rects1 = f2_ax2.bar(x, acc_list, width=0.3)\n",
    "   # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "   f2_ax2.set_xlabel('Party ' + str(party_id))\n",
    "   f2_ax2.set_ylabel('Average Prediction Accuracy \\n over '+str(sample_count)+' samples')\n",
    "   f2_ax2.set_title('Average Prediction Accuracy by parties for label '+str(i))\n",
    "   f2_ax2.set_xticks(x)\n",
    "   f2_ax2.set_xticklabels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut Down\n",
    "\n",
    "Invoke the `stop()` method on each of the network participants to terminate the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
